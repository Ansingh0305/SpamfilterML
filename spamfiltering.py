# -*- coding: utf-8 -*-
"""SpamFiltering.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wDLahEHZJacDjJrOrQzS-Q7BtPwLKSc3
"""

import numpy as np
import pandas as pd

df = pd.read_csv('spam_utf8.csv')

df.sample(5)

##!. Data Cleaning

df.info()

##Drop last 3 columns
df.drop(columns=['Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4'], inplace=True)

df.sample(5)

df.rename(columns={'v1':'target', 'v2':'text'}, inplace=True)
df.sample(5)

##converting the ham and spam into the form of 1 and 0
from sklearn.preprocessing import LabelEncoder
encoder = LabelEncoder()

df['target'] = encoder.fit_transform(df[ 'target'])

df.sample(5)

df.head()

#checking for missing values
df.isnull().sum()

#checking for duplicated

df.duplicated().sum()

#removing duplicated
df = df.drop_duplicates(keep='first')

df.duplicated().sum()

df.shape

df.shape

##EDA Exploratory Data Analysis

df['target'].value_counts()

import matplotlib.pyplot as plt
plt.pie(df['target'].value_counts(), labels=['ham', 'spam'], autopct="%0.2f")
#here autopct = %0.2f means we can see the percentages to 2 decimal places
plt.show()

#Data is imbalanced :

#In machine learning, data is considered "balanced" when the number of instances of each class is roughly equal or nearly equal. In other words, the distribution of instances across classes is even or nearly even.

#On the other hand, data is considered "imbalanced" when the number of instances of one class significantly outnumbers the number of instances of another class. This results in an uneven distribution of instances across classes.

#Imbalanced data can have a negative impact on machine learning models, as they may be biased towards the majority class and perform poorly on the minority class. This is because machine learning algorithms are often designed to maximize overall accuracy, which can be misleading in the presence of imbalanced data.

#To address the issue of imbalanced data, various techniques can be used, such as oversampling the minority class, undersampling the majority class, or using a combination of both. Another approach is to use specialized algorithms that are designed to handle imbalanced data, such as SMOTE (Synthetic Minority Over-sampling Technique) or cost-sensitive learning.

#Balancing the data can improve the performance of machine learning models by ensuring that they have a sufficient number of instances of each class to learn from. However, it is important to note that balancing the data may not always be possible or desirable, as it may result in the loss of valuable information. In such cases, it is important to carefully evaluate the performance of the model and consider alternative approaches.

import nltk  #natural language tool kit

nltk.download('punkt_tab') #donwloading dependencies

df['num_characters'] = df['text'].apply(len)
#created a new column containing the length of the sms
#n pandas, the apply() function is a method that can be used to apply a given function to each row or column of a DataFrame or Series. It allows you to perform operations on the data in a more flexible and efficient way, without having to write explicit loops.

df.head()

#number of words
#A lambda function is a small anonymous function that can be defined using the lambda keyword. It is often used as an alternative to defining a separate function, when the function is only used once or in a concise way.
df['num_words'] = df['text'].apply(lambda x:len(nltk.word_tokenize(x)))
#word_tokenize tokenizes on basis of words

df.head()

#sent_tokenize tokenizes on the basis of sentences
#you can see the sentences it has tokenized by removing the 'len' below
df['num_sentences'] = df['text'].apply(lambda x:len(nltk.sent_tokenize(x)))

df.head()

df[['num_characters', 'num_words', 'num_sentences']].describe()

#ham messages
 df[df['target']==0][['num_characters', 'num_words', 'num_sentences']].describe()

#spam messages
df[df['target']==1][['num_characters', 'num_words', 'num_sentences']].describe()

#we can see that the avg spam massages have more characters and words and sentences as compared to the ham messages

import seaborn as sns
#using seaborn to visualise the data

plt.figure(figsize=(12,6))
sns.histplot(df[df['target']==0]['num_characters'])
sns.histplot(df[df['target']==1]['num_characters'], color = 'red')

plt.figure(figsize=(12,6))
sns.histplot(df[df['target']==0]['num_words'])
sns.histplot(df[df['target']==1]['num_words'], color = 'red')

#as you can see, wherever the number of words or characters are more,the count of spams are more

sns.pairplot(df, hue='target')



corr_matrix = df.drop(columns=['text'])
sns.heatmap(corr_matrix.corr(), annot=True)

#this corr function only requires the numeric values hence we made a new list without the text column and then used the corr function to show it
#this corr function gives the correlation factor showing how much a class depends on another class
#and through the heatmap we can see the denseness

#here above the target has a 0.38 correlation with num_characters which means that more the number of characters more is the tendency of the spam to be classified into 1
#we have to see the correlation of target with the possible columns, and we can see that the different properties are very strongly correlated with each other
#if there are some properties which are strongly correlated with each other, we generally avoid using those as features as they may lead to overfitting and other problems
#Hence we will only be using the correlation of target with num_characters since that is highly correlated with it and we wont use other properties as they are highly correlated with each other

##Data Processing
#1. Lower Case
#2. Tokenization (converting into words)
#3. Removing Special Characters
#4. Removing Stop words and punctuation
#5. Stemming

from nltk.corpus import stopwords

nltk.download('stopwords')
import string

from nltk.stem import PorterStemmer
ps = PorterStemmer()

def transform_text(text):
  text = text.lower()
  text = nltk.word_tokenize(text)

  y = []

  for i in text:
    if i.isalnum():
      y.append(i)

  text = y[:]
  y.clear()

  for i in text:
    if i not in stopwords.words('english') and i not in string.punctuation:
      y.append(i)

  text = y[:]
  y.clear()

  for i in text:
    y.append(ps.stem(i))

  return " ".join(y)

transform_text('hello how are you')

df['transformed_text'] = df['text'].apply(transform_text)

df.head()

#now showing the major words which contribute to a spam sms :
from wordcloud import WordCloud
wc = WordCloud(width=500, height=500, min_font_size=10, background_color='white')

spam_wc = wc.generate(df[df['target']==1]['transformed_text'].str.cat(sep=" "))

plt.imshow(spam_wc)

ham_ws = wc.generate(df[df['target']==0]['transformed_text'].str.cat(sep=" "))

plt.imshow(ham_ws)

df.head(
)

spam_corpus = []
for i in df[df['target']==1]['transformed_text'].tolist():
  for word in i.split():
    spam_corpus.append(word)

ham_corpus = []
for msg in df[df['target']==0]['transformed_text'].tolist():
  for word in msg.split():
    ham_corpus.append(word)

from collections import Counter
Counter(ham_corpus).most_common(30)

##Model Building

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfVectorizer
cv = CountVectorizer() #CountVectorizer is used to transform a given text into a vector on the basis of the frequency (count) of each word that occurs in the entire text.
tfidf = TfidfVectorizer(max_features=3000) #This type of vectorizer works better in our case, it maps how frequent the words are and how rare they are where tf = term frequency and idf = inverse document frequency

# X = cv.fit_transform(df['transformed_text']).toarray()
#here cv.fit_tranform converted to sparse array which got converted to dense array by the toarray() function.
#the tfidf is working better hence we are using that instead
X = tfidf.fit_transform(df['transformed_text']).toarray()
#here we tried scaling using MinMaxScaler but that did not provide good results by decreasing the precision

X

X.shape

#we have the x and now we are extracting y

y = df['target'].values

#now we are going to perform the tests for training and will be using a library function train_test_split which will split the data into two parts and take some part for testing and some part for training
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2)

from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB #these are different types of naive bayes algorithm and since we dont know what type of data we are getting hence we are using all of them
from sklearn.metrics import accuracy_score, precision_score, confusion_matrix  #these are the tools which will help us understand if the model is working fine or not

gnb = GaussianNB()
mnb = MultinomialNB()
bnb = BernoulliNB()

gnb.fit(X_train, y_train)
y_pred = gnb.predict(X_test)
print(accuracy_score(y_test, y_pred))
print(confusion_matrix(y_test, y_pred))
print(precision_score(y_test, y_pred))

mnb.fit(X_train, y_train)
y_pred2 = mnb.predict(X_test)
print(accuracy_score(y_test, y_pred2))
print(confusion_matrix(y_test, y_pred2))
print(precision_score(y_test, y_pred2))
#got a precision score of 1, which matters more here hence we will be going on with the MultinomialNB
#Why is Precision More Important than Accuracy?
#Accuracy is misleading in imbalanced data
#If a model predicts everything as "not spam", accuracy = 95%
#→ But it never detects spam ❌
# Precision focuses on reducing False Positives (FP)

bnb.fit(X_train, y_train)
y_pred3 = bnb.predict(X_test)
print(accuracy_score(y_test, y_pred3))
print(confusion_matrix(y_test, y_pred3))
print(precision_score(y_test, y_pred3))
#Could be an option but since the data is imbalanced hence precision score matters more

#tfidf -->  mnb

pd.DataFrame({'Accuracy':accuracy_score(y_test, y_pred2), 'Precision':precision_score(y_test, y_pred2)}, index=['MultinomialNB'])
#

##Improving the Model :

 #in order to improve we tried taking the first some thousand words with the highest frequencies only in order to train the model and not use the less frequencies one
 #in order to do so i have put max_features=3000
 #the precision of mnb is still 1 and the accuracy has increased
 #We also tried scaling with MinMaxScaler (not standardScaler because that also gave negative values which are not acceptable by naive bayes)
 #but this scaling lead to reduction in precision and hence we are not integrating scaling into our algo
 #we also tried adding the characters column in the training data to see if changes take place but it lead to negative effects and hence it still didnt work
 #we also tried the combination of different best perfoming algorithms but it still didnt give better results than simple Multinomial Naive Bayes
 #hence we plan on using the simple Multinomial Naive Bayes

#now to make a pipeline to process the new input and see if our model is working correclty:

import pickle
pickle.dump(tfidf, open('vectorizer.pkl', 'wb'))
pickle.dump(mnb, open('model.pkl', 'wb'))
#pickle helps us to store objects that we have created and trained and stored different values in:
#this leads to the creation of two files which then we can copy and paste and those can be used anywhere
#so basically we have the whole model with us now in the files vectorizer.pkl and model.pkl

